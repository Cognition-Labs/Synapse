{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import dotenv\n",
    "import modal\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.llms import OpenAI\n",
    "from langchain import PromptTemplate, LLMChain\n",
    "\n",
    "PERSIST_DIR = \"embeddings\"\n",
    "DOT_ENV_PATH = \".env\"\n",
    "\n",
    "dotenv.load_dotenv(DOT_ENV_PATH)\n",
    "OPENAI_API_KEY = os.environ.get('OPENAI_API_KEY')\n",
    "\n",
    "image = modal.Image.debian_slim().pip_install_from_requirements(\"requirements.txt\")\n",
    "\n",
    "mounts = [\n",
    "    modal.Mount.from_local_file(\".env\", remote_path=DOT_ENV_PATH),\n",
    "    modal.Mount.from_local_dir(\"embeddings\", remote_path=PERSIST_DIR),\n",
    "]\n",
    "\n",
    "stub = modal.Stub(name=\"Synapse\", mounts=mounts, image=image)\n",
    "\n",
    "def load_vectordb(db_name):\n",
    "    embedding = OpenAIEmbeddings(disallowed_special=())\n",
    "    vectordb = Chroma(persist_directory=os.path.join(PERSIST_DIR, db_name), embedding_function=embedding)\n",
    "    return vectordb.as_retriever()\n",
    "\n",
    "def create_prompt_template():\n",
    "    template = \"\"\"\n",
    "    You are a creativity engine. You will show a reduced representation of a corpus to draw out connections with reference to the original query of your pupil.\n",
    "\n",
    "    For example, the following is the query:\n",
    "    \"{query}\"\n",
    "\n",
    "    The following is the corpus:\n",
    "    \"{corpus}\"\n",
    "\n",
    "    Use extreme patchwriting and mosaic writing as your style, maximally using exact quotations. Use quotes (\"\") and ellipses (...) to delineate between quotations of the corpus.\n",
    "    Use the quotations as your canvas instead of using your own words. Do this to help the pupil have creative thoughts based on a reduced representation of the original text with reference to the given query.\n",
    "    Only give the resultant patchwork and no extra explanations. Let your work speak for itself.\n",
    "    \"\"\"\n",
    "    return PromptTemplate(template=template, input_variables=['query','corpus'])\n",
    "\n",
    "def create_llm_chain(prompt):\n",
    "    llm = OpenAI(model='gpt-3.5-turbo')\n",
    "    return LLMChain(prompt=prompt, llm=llm)\n",
    "\n",
    "def process_documents(docs, llm_chain, query):\n",
    "    for doc in docs:\n",
    "        corpus = doc['page_content']\n",
    "        input = {'query' : query, 'corpus' : corpus}\n",
    "        output = llm_chain.run(input)\n",
    "        output = output.replace(\"\\n\", \" \").replace('\"', \"\").replace(\"'\", \"\")\n",
    "        doc['page_content'] = output\n",
    "\n",
    "@stub.function(cpu=2, memory=2048, container_idle_timeout=300, keep_warm=1)\n",
    "@modal.web_endpoint(method=\"POST\")\n",
    "def run_query(query: str, db_name: str):\n",
    "    retriever = load_vectordb(db_name)\n",
    "    docs = retriever.get_relevant_documents(query=query)\n",
    "    docs = [json.loads(doc.json()) for doc in docs]\n",
    "    prompt = create_prompt_template()\n",
    "    llm_chain = create_llm_chain(prompt)\n",
    "    process_documents(docs, llm_chain, query)\n",
    "    return docs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "synapse",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
