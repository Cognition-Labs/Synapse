{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "import os\n",
    "\n",
    "import fitz\n",
    "from PIL import Image\n",
    "import openai\n",
    "import dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "DOT_ENV_PATH = \"./.env\"\n",
    "dotenv.load_dotenv(DOT_ENV_PATH)\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_file(file):\n",
    "    # raise an error if API key is not provided\n",
    "    if 'OPENAI_API_KEY' not in os.environ:\n",
    "        print('Upload your OpenAI API key')\n",
    "        return None\n",
    "    \n",
    "    # Load the PDF file using PyPDFLoader\n",
    "    loader = PyPDFLoader(file) \n",
    "    documents = loader.load()\n",
    "    \n",
    "    # Initialize OpenAIEmbeddings for text embeddings\n",
    "    embeddings = OpenAIEmbeddings()\n",
    "    \n",
    "    # Create a ConversationalRetrievalChain with ChatOpenAI language model\n",
    "    # and PDF search retriever\n",
    "    pdfsearch = FAISS.from_documents(documents, embeddings,)\n",
    "\n",
    "    chain = ConversationalRetrievalChain.from_llm(ChatOpenAI(temperature=0.3), \n",
    "                                                  retriever=\n",
    "                                                  pdfsearch.as_retriever(search_kwargs={\"k\": 1}),\n",
    "                                                  return_source_documents=True,)\n",
    "    return chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ConversationalRetrievalChain(memory=None, callbacks=None, callback_manager=None, verbose=False, tags=None, combine_docs_chain=StuffDocumentsChain(memory=None, callbacks=None, callback_manager=None, verbose=False, tags=None, input_key='input_documents', output_key='output_text', llm_chain=LLMChain(memory=None, callbacks=None, callback_manager=None, verbose=False, tags=None, prompt=ChatPromptTemplate(input_variables=['context', 'question'], output_parser=None, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context'], output_parser=None, partial_variables={}, template=\"Use the following pieces of context to answer the users question. \\nIf you don't know the answer, just say that you don't know, don't try to make up an answer.\\n----------------\\n{context}\", template_format='f-string', validate_template=True), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['question'], output_parser=None, partial_variables={}, template='{question}', template_format='f-string', validate_template=True), additional_kwargs={})]), llm=ChatOpenAI(verbose=False, callbacks=None, callback_manager=None, tags=None, client=<class 'openai.api_resources.chat_completion.ChatCompletion'>, model_name='gpt-3.5-turbo', temperature=0.3, model_kwargs={}, openai_api_key='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', openai_api_base='', openai_organization='', openai_proxy='', request_timeout=None, max_retries=6, streaming=False, n=1, max_tokens=None), output_key='text', output_parser=NoOpOutputParser(), return_final_only=True, llm_kwargs={}), document_prompt=PromptTemplate(input_variables=['page_content'], output_parser=None, partial_variables={}, template='{page_content}', template_format='f-string', validate_template=True), document_variable_name='context', document_separator='\\n\\n'), question_generator=LLMChain(memory=None, callbacks=None, callback_manager=None, verbose=False, tags=None, prompt=PromptTemplate(input_variables=['chat_history', 'question'], output_parser=None, partial_variables={}, template='Given the following conversation and a follow up question, rephrase the follow up question to be a standalone question, in its original language.\\n\\nChat History:\\n{chat_history}\\nFollow Up Input: {question}\\nStandalone question:', template_format='f-string', validate_template=True), llm=ChatOpenAI(verbose=False, callbacks=None, callback_manager=None, tags=None, client=<class 'openai.api_resources.chat_completion.ChatCompletion'>, model_name='gpt-3.5-turbo', temperature=0.3, model_kwargs={}, openai_api_key='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', openai_api_base='', openai_organization='', openai_proxy='', request_timeout=None, max_retries=6, streaming=False, n=1, max_tokens=None), output_key='text', output_parser=NoOpOutputParser(), return_final_only=True, llm_kwargs={}), output_key='answer', return_source_documents=True, return_generated_question=False, get_chat_history=None, retriever=VectorStoreRetriever(vectorstore=<langchain.vectorstores.faiss.FAISS object at 0x127c88a10>, search_type='similarity', search_kwargs={'k': 1}), max_tokens_limit=None)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "process_file('/Users/danielgeorge/Documents/work/ml/hypolab/Synapse/server/data/Radin - 2023 - Sentiment and Presentiment in Twitter Do Trends in Collective Mood “Feel the Future”.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_response(history, query, btn):\n",
    "    global COUNT, N, chat_history\n",
    "    \n",
    "    # Check if a PDF file is uploaded\n",
    "    if not btn:\n",
    "        raise gr.Error(message='Upload a PDF')\n",
    "    \n",
    "    # Initialize the conversation chain only once\n",
    "    if COUNT == 0:\n",
    "        chain = process_file(btn)\n",
    "        COUNT += 1\n",
    "    \n",
    "    # Generate a response using the conversation chain\n",
    "    result = chain({\"question\": query, 'chat_history':chat_history}, return_only_outputs=True)\n",
    "    \n",
    "    # Update the chat history with the query and its corresponding answer\n",
    "    chat_history += [(query, result[\"answer\"])]\n",
    "    \n",
    "    # Retrieve the page number from the source document\n",
    "    N = list(result['source_documents'][0])[1][1]['page']\n",
    "\n",
    "    # Append each character of the answer to the last message in the history\n",
    "    for char in result['answer']:\n",
    "        history[-1][-1] += char\n",
    "        \n",
    "        # Yield the updated history and an empty string\n",
    "        yield history, ''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HTML"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "synapse",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
